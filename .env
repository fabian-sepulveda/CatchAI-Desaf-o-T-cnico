# FRONTEND -> apunta al backend
BACKEND_URL=http://host.docker.internal:8000
# En Linux, pref: BACKEND_URL=http://backend:8000

# BACKEND -> proveedor de LLM/embeddings
PROVIDER=ollama
OLLAMA_BASE_URL=http://host.docker.internal:11434   # Windows/Mac con Ollama en host
# Linux: OLLAMA_BASE_URL=http://host.docker.internal:11434 (funciona en Docker Desktop)
# Si no funciona en Linux: usa http://localhost:11434 y network_mode: host (ver nota abajo)

OLLAMA_MODEL=mistral
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# HuggingFace embeddings (opcional)
USE_HF_EMBEDDINGS=true
HF_EMBEDDINGS_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Vector DB
CHROMA_BASE_DIR=/app/data/chroma

# Límites (opcional)
MAX_FILES=0           # 0 = sin límite
MAX_TOTAL_MB=0
